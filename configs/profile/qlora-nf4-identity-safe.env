PEFT=lora
PEFT_CONFIG='{"r":8,"lora_alpha":32,"lora_dropout":0.1}'

QUANT=nf4
PRECISION=bf16
STRATEGY=deepspeed_stage_1
OP=cuda

MICRO_BSZ=1
ACCUMULATE_GRAD_BATCHES=8
GRAD_CP=1

EPOCH_STEPS=48
EPOCH_COUNT=1
EPOCH_SAVE=1

LR_INIT=5e-6
LR_FINAL=5e-6

TRAIN_TYPE=none
CHUNK_CTX=512
FUSED_KERNEL=0
