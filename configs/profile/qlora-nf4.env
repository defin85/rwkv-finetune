PEFT=lora
PEFT_CONFIG='{"r":8,"lora_alpha":32,"lora_dropout":0.05}'

QUANT=nf4
PRECISION=bf16
STRATEGY=deepspeed_stage_1
OP=cuda

MICRO_BSZ=1
ACCUMULATE_GRAD_BATCHES=16
GRAD_CP=1

EPOCH_STEPS=400
EPOCH_COUNT=3
EPOCH_SAVE=1

LR_INIT=1e-5
LR_FINAL=1e-5

TRAIN_TYPE=none
CHUNK_CTX=512
FUSED_KERNEL=0

