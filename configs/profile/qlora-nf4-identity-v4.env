PEFT=lora
PEFT_CONFIG='{"r":32,"lora_alpha":64,"lora_dropout":0.05}'

QUANT=nf4
PRECISION=bf16
STRATEGY=deepspeed_stage_1
OP=cuda

MICRO_BSZ=1
ACCUMULATE_GRAD_BATCHES=8
GRAD_CP=1

EPOCH_STEPS=160
EPOCH_COUNT=2
EPOCH_SAVE=1

LR_INIT=1.2e-5
LR_FINAL=8e-6

TRAIN_TYPE=none
CHUNK_CTX=512
FUSED_KERNEL=0
