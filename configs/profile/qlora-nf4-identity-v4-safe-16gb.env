PEFT=lora
PEFT_CONFIG='{"r":16,"lora_alpha":32,"lora_dropout":0.05}'

QUANT=nf4
PRECISION=bf16
STRATEGY=deepspeed_stage_1
OP=cuda

MICRO_BSZ=1
ACCUMULATE_GRAD_BATCHES=16
GRAD_CP=1

EPOCH_STEPS=96
EPOCH_COUNT=1
EPOCH_SAVE=1

LR_INIT=7e-6
LR_FINAL=6e-6

TRAIN_TYPE=none
CHUNK_CTX=512
FUSED_KERNEL=0
